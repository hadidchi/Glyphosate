{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JowdR0AoL_eS"
      },
      "outputs": [],
      "source": [
        "pip install linearmodels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from linearmodels.panel import PanelOLS"
      ],
      "metadata": {
        "id": "L9KA8PnlQXBx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('/content/Glyphosate Suicide Dataset.csv')"
      ],
      "metadata": {
        "id": "UqbE-C8iQyfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = dataset.copy()\n",
        "df.columns = (df.columns.str.strip().str.replace(r\"[^\\w]+\",\"_\", regex=True).str.replace(r\"_$\",\"\", regex=True))\n",
        "df[\"County_Code\"] = df[\"County_Code\"].astype(str)\n",
        "df[\"Year\"] = pd.to_numeric(df[\"Year\"], errors=\"coerce\").astype(\"Int64\")\n",
        "df = df[df['Unemployment_Rate']!='N.A.'].reset_index(drop=True)\n",
        "df[\"glyph_q\"] = (df.groupby(\"Year\")[\"Glyphosate_kg_per_km2\"].transform(lambda x: pd.qcut(x, 5, labels=[\"Q1\",\"Q2\",\"Q3\",\"Q4\",\"Q5\"], duplicates=\"drop\")))\n",
        "df[\"glyph_q_ord\"] = df[\"glyph_q\"].astype(str).str.replace(\"Q\", \"\", regex=False)\n",
        "df[\"glyph_q_ord\"] = pd.to_numeric(df[\"glyph_q_ord\"], errors=\"coerce\")\n",
        "\n",
        "OUTCOMES = {\"all\": \"Suicide_Death_Rate\", \"firearm\": \"Firearm_Suicide_Death_Rate\", \"nonfirearm\": \"Non_Firearm_Suicide_Death_Rate\",}\n",
        "\n",
        "base_covars = [\"Mean_Age\", \"Female\", \"American_Indian_or_Alaska_Native\", \"Asian_or_Pacific_Islander\", \"Black_or_African_American\", \"Hispanic_or_Latino\", \"Total_Alcohol\", \"Unemployment_Rate\", \"PM2_5\", \"Urbanicity\",]\n",
        "\n",
        "def _build_X(d, use_trend=False):\n",
        "    # exposure: either quintile dummies (Q1 ref) or ordinal trend term\n",
        "    if use_trend:\n",
        "        X = pd.DataFrame({\"glyph_trend\": d[\"glyph_q_ord\"].astype(float)}, index=d.index)\n",
        "    else:\n",
        "        X = pd.get_dummies(d[\"glyph_q\"], prefix=\"glyph\", drop_first=True)  # glyph_Q2..glyph_Q5\n",
        "\n",
        "    # numeric covariates\n",
        "    X = pd.concat([X, d[[\"Mean_Age\",\"Female\", \"American_Indian_or_Alaska_Native\",\"Asian_or_Pacific_Islander\",\"Black_or_African_American\", \"Hispanic_or_Latino\", \"Total_Alcohol\",\"Unemployment_Rate\",\"PM2_5\"]]], axis=1)\n",
        "\n",
        "    # urbanicity dummies\n",
        "    X = pd.concat([X, pd.get_dummies(d[\"Urbanicity\"].astype(str), prefix=\"Urban\", drop_first=True)], axis=1)\n",
        "    return X.loc[:, X.nunique(dropna=True) > 1]\n",
        "\n",
        "def fit_panel_loglinear(rate_col, eps=1e-9, trend=False):\n",
        "    d = df.copy()\n",
        "    d[rate_col] = pd.to_numeric(d[rate_col], errors=\"coerce\")\n",
        "    d[\"Population\"] = pd.to_numeric(d[\"Population\"], errors=\"coerce\")\n",
        "\n",
        "    req = [\"County_Code\",\"Year\",\"Population\",rate_col] + base_covars + ([\"glyph_q_ord\"] if trend else [\"glyph_q\"])\n",
        "    d = d.dropna(subset=req).copy()\n",
        "\n",
        "    d[\"y\"] = np.log(d[rate_col].astype(float) + eps)\n",
        "    d = d.set_index([\"County_Code\",\"Year\"]).sort_index()\n",
        "\n",
        "    X = _build_X(d, use_trend=trend)\n",
        "\n",
        "    mod = PanelOLS(\n",
        "        d[\"y\"], X,\n",
        "        entity_effects=True,\n",
        "        time_effects=True,\n",
        "        weights=d[\"Population\"],\n",
        "        drop_absorbed=True\n",
        "    )\n",
        "    return mod.fit(cov_type=\"clustered\", cluster_entity=True)\n",
        "\n",
        "def pct_change_table(res):\n",
        "    params, ses, pvals = res.params, res.std_errors, res.pvalues\n",
        "    q_terms = [t for t in params.index if t.startswith(\"glyph_\")]  # glyph_Q2..glyph_Q5\n",
        "    out = pd.DataFrame({\"beta\": params[q_terms], \"pct\": 100*(np.exp(params[q_terms]) - 1), \"pct_lo\": 100*(np.exp(params[q_terms] - 1.96*ses[q_terms]) - 1), \"pct_hi\": 100*(np.exp(params[q_terms] + 1.96*ses[q_terms]) - 1), \"p\": pvals[q_terms],})\n",
        "    out.index = out.index.str.replace(\"glyph_\", \"\", regex=False)  # Q2..Q5\n",
        "    return out.sort_index()\n",
        "\n",
        "def ptrend_from_res(res_trend):\n",
        "    # p-trend is the p-value of the single ordinal term\n",
        "    term = \"glyph_trend\"\n",
        "    return float(res_trend.pvalues[term]) if term in res_trend.pvalues.index else np.nan\n",
        "\n",
        "# --- run outcomes: main model + p-trend model ---\n",
        "results, tables, ptrend = {}, {}, {}\n",
        "\n",
        "for k, col in OUTCOMES.items():\n",
        "    res = fit_panel_loglinear(col, trend=False)\n",
        "    res_trend = fit_panel_loglinear(col, trend=True)\n",
        "\n",
        "    results[k] = res\n",
        "    tables[k] = pct_change_table(res)\n",
        "    ptrend[k] = ptrend_from_res(res_trend)\n",
        "\n",
        "    print(\"\\n===============================\")\n",
        "    print(f\"Outcome: {k} ({col})\")\n",
        "    print(res.summary)\n",
        "    print(\"\\nQuintile % change vs Q1:\")\n",
        "    print(tables[k])\n",
        "    print(f\"\\nP-trend (ordinal Q1â€“Q5): {ptrend[k]:.6g}\")"
      ],
      "metadata": {
        "id": "j1SjtZCiMD_H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}